{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0a0cff",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7d1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# install Pytorch in Python\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20f4d4",
   "metadata": {},
   "source": [
    "### Tensors \n",
    "Tensor data structure is almost exactly like numpy arrays. The only difference is tensors can also be loaded onto GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e02d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "np_array = np.array(data)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed9327c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.from_numpy(np_array)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10927155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c71665d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also create a tensor directly \n",
    "tensor2 = torch.tensor([1, 2, 3, 4])\n",
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba0933b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check which device it's on\n",
    "tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4309e1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast back into numpy array \n",
    "np_array_back = tensor.detach().numpy() #.detach() is necessary if its on a GPU\n",
    "np_array_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38c368",
   "metadata": {},
   "source": [
    "### Logistic regression with Pytorch\n",
    "\n",
    "In HW3, you implemnted binary logistic regression using numpy, let's do it with Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164ee301",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2 #let's do binary logistic regression again \n",
    "\n",
    "# Rows: Examples \n",
    "# Columns: Features \n",
    "X_train = torch.tensor([[1.0, 0.0, 0.0], \n",
    "                        [0.0, 1.0, 1.0], \n",
    "                        [1.0, 1.0, 0.0], \n",
    "                        [0.0, 0.0, 1.0]])\n",
    "\n",
    "Y_train = torch.tensor([0, 1, 0, 1])\n",
    "\n",
    "num_examples = X_train.shape[0]\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "assert X_train.shape[0] == Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e215598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear layer (this is an object!)\n",
    "# The object holds the internal state (weights, how those weights are manipulated)\n",
    "# y = Wx + b\n",
    "# Pytorch automatically initializes the weights randomly \n",
    "theta = nn.Linear(num_features, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3a3181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=2, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73909923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.linear.Linear"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc9293d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the forward method \n",
    "out = theta.forward(X_train)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85bbc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This notation is shortened! We don't have to call the forward\n",
    "out = theta(X_train)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe858ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7654, -0.0908],\n",
       "        [-0.1242, -0.5379],\n",
       "        [-0.3477, -0.5586],\n",
       "        [-0.5419, -0.0702]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "535157ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log softmax function (for numerical stability)\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "log_probs = log_softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "138d5b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0862267 , -0.41171592],\n",
       "       [-0.5075397 , -0.92124003],\n",
       "       [-0.5932447 , -0.8041493 ],\n",
       "       [-0.95656514, -0.48484987]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get back to numpy \n",
    "log_probs_numpy = log_probs.detach().numpy()\n",
    "log_probs_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c010d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33748755, 0.6625125 ],\n",
       "       [0.6019748 , 0.39802518],\n",
       "       [0.5525316 , 0.4474684 ],\n",
       "       [0.38421032, 0.61578965]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How do we get probabilities out?\n",
    "np.exp(log_probs_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a2296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's only get the positive class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84fab00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6625125 , 0.39802518, 0.4474684 , 0.61578965], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_pos_class = np.exp(log_probs_numpy)[:,1] # all rows, only second column\n",
    "pred_pos_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc5d29e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (pred_pos_class > 0.5).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaefd060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that it works on a toy example \n",
    "pred_pos_class = np.array([0.6, 0.2])\n",
    "y_pred = (pred_pos_class > 0.5).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c84b1a",
   "metadata": {},
   "source": [
    "Let's put this back into the class strucutre we used for HW 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3df7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionModel(nn.Module): #We say that it is class is inheriting from the nn.Module class, \n",
    "                                                #which is a built-in PyTorch class\n",
    "    \"\"\"\n",
    "    Pytorch implementation for binary logistic regression \n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Linear(num_features, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.theta(X)\n",
    "        log_probs = self.log_softmax(out)\n",
    "        return log_probs\n",
    "        \n",
    "    def predict(self, X): \n",
    "        self.eval()\n",
    "        pred_log_probs = self.forward(X)\n",
    "        log_probs_numpy = log_probs.detach().numpy()\n",
    "        pred_pos_class = np.exp(log_probs_numpy)[:, 1]\n",
    "        y_pred = (pred_pos_class > 0.5).astype(int)\n",
    "        return y_pred, pred_pos_class   \n",
    "    \n",
    "    def train_model(self, X, Y, loss_fn, optimizer, num_iterations):\n",
    "        # We'll cover this on Tuesday \n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14da65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model \n",
    "model = BinaryLogisticRegressionModel(num_features, NUM_CLASSES) #initalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55815468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference time (predict on a single example) \n",
    "X_test = torch.tensor([[1.0, 0.0, 0.0]])\n",
    "y_pred, pred_pos_class = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "074783a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: BinaryLogisticRegressionModel(\n",
      "  (theta): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "\n",
      "Layer: theta.weight | Size: torch.Size([2, 3]) | Values : tensor([[ 0.4426,  0.3318, -0.1153],\n",
      "        [ 0.3504,  0.0606, -0.0538]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: theta.bias | Size: torch.Size([2]) | Values : tensor([0.0197, 0.2193], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can inspect our model here too \n",
    "print(f\"Model structure: {model}\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbed9cb",
   "metadata": {},
   "source": [
    "### More Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c211f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicFeedForwardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation for single hidden layer deep learning model \n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_classes, hidden_dim1):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(num_features, hidden_dim1)\n",
    "        self.theta = nn.Linear(hidden_dim1, num_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        hid1 = nn.functional.relu(self.hidden1(X))\n",
    "        out = self.theta(hid1)\n",
    "        log_probs = self.log_softmax(out)\n",
    "        return log_probs\n",
    "        \n",
    "    def predict(self, X): \n",
    "        self.eval()\n",
    "        pred_log_probs = self.forward(X)\n",
    "        log_probs_numpy = log_probs.detach().numpy()\n",
    "        pred_pos_class = np.exp(log_probs_numpy)[:, 1]\n",
    "        y_pred = (pred_pos_class > 0.5).astype(int)\n",
    "        return y_pred, pred_pos_class    \n",
    "    \n",
    "    def train_model(self, X, Y, loss_fn, optimizer, num_iterations): \n",
    "        # We'll cover this on Tuesday\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20c2ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 5\n",
    "model_ff = BasicFeedForwardModel(num_features, NUM_CLASSES, HIDDEN_DIM) #initalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4b9e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([[1.0, 0.0, 0.0]])\n",
    "y_pred, pred_pos_class = model_ff.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f6be9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also save and load the models (useful for long trainng)\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f173e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model \n",
    "model = torch.load('model.pth', weights_only=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
