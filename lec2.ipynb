{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abe6339",
   "metadata": {},
   "source": [
    "# Lecture 2: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289d42e",
   "metadata": {},
   "source": [
    "### Regex for tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c611796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e17e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good muffins cost $3.88\n",
      "in New York.  Please buy me\n",
      "two of them!\n",
      "\n",
      "Thanks!\n"
     ]
    }
   ],
   "source": [
    "text = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them!\\n\\nThanks!\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e297c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " 'Thanks']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = r'[a-zA-z]+'\n",
    "re.findall(regex, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6341f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '3',\n",
       " '88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " 'Thanks']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = r'\\w+'\n",
    "re.findall(regex, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802eab95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them!',\n",
       " 'Thanks!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = r'\\S+'\n",
    "re.findall(regex, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147b45d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '!',\n",
       " 'Thanks',\n",
       " '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final (pretty good) tokenization\n",
    "regex = r'\\w+|\\$[\\d\\.]+|\\S+'\n",
    "re.findall(regex, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502d491",
   "metadata": {},
   "source": [
    "### BPE algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c15a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "# Special subclasses of the \"collections\" class\n",
    "# Not primitive types but behave like dictionaries with additional functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756557e",
   "metadata": {},
   "source": [
    "`defaultdict` is a subclass of the built-in dict. It provides a default value for a nonexistent key, allowing you to avoid `KeyError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d30292",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7eb7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'a': [1]})\n"
     ]
    }
   ],
   "source": [
    "dd['a'].append(1)\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5c20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 1})\n"
     ]
    }
   ],
   "source": [
    "cc = Counter()\n",
    "cc['a'] += 1\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191f20c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g r e a t'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"great\"\n",
    "chars = ' '.join(list(word))\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cab3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.current_corpus = Counter()\n",
    "        \n",
    "    def init_vocab(self, corpus: dict):\n",
    "        \"\"\" Count the frequency of each character \"\"\"\n",
    "        \n",
    "        for word, freq in corpus.items():\n",
    "            # Corpus split into characters \n",
    "            self.current_corpus[\" \".join(list(word))] += freq \n",
    "            \n",
    "            # Character counts in vocab \n",
    "            for char in list(word): \n",
    "                self.vocab.add(char)\n",
    "\n",
    "    def get_stats(self):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in self.current_corpus.items():\n",
    "            toks = word.split(\" \") #splits into tokens we've already made \n",
    "            for i in range(len(toks) - 1):\n",
    "                pairs[(toks[i], toks[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair: tuple):\n",
    "        # Update vocab \n",
    "        replacement = ''.join(pair)\n",
    "        self.vocab.add(replacement)\n",
    "        \n",
    "        # Update corpus \n",
    "        new_corpus = {}\n",
    "        bigram = ' '.join(pair) #previous representation \n",
    "        for word in self.current_corpus:\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_corpus[new_word] = self.current_corpus[word] # update freq\n",
    "            \n",
    "        self.current_corpus = new_corpus \n",
    "\n",
    "    def token_learner(self, corpus: dict, k: int, verbose=True):\n",
    "        \"\"\"\n",
    "        Here, k is the number of merges\n",
    "        \"\"\"\n",
    "        self.init_vocab(corpus)\n",
    "        if verbose: \n",
    "            print(\"Step 0\")\n",
    "            print(\"Vocab: \", self.vocab)\n",
    "            print(\"Corpus: \", self.current_corpus)\n",
    "            print(\"====\"*15)\n",
    "\n",
    "        for i in range(k):\n",
    "            pairs = self.get_stats()\n",
    "            if not pairs: break #empty merges \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            the_count = pairs[best_pair]\n",
    "            self.merge_vocab(best_pair)\n",
    "            \n",
    "            if verbose: \n",
    "                print(f\"Step {i + 1}: Merged {best_pair} -> {''.join(best_pair)}, Count:{the_count}\")\n",
    "                print(\"Vocab: \", self.vocab)\n",
    "                print(\"Corpus: \", self.current_corpus)\n",
    "                print(\"====\"*15)\n",
    "            input(\"Next? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e2e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Vocab:  {'d', 'i', 's', 'n', 'o', 'w', 'l', 't', 'e', 'r', '_'}\n",
      "Corpus:  Counter({'n e w e r _': 6, 'l o w _': 5, 'w i d e r _': 3, 'l o w e s t _': 2, 'n e w _': 2})\n",
      "============================================================\n",
      "Step 1: Merged ('e', 'r') -> er, Count:9\n",
      "Vocab:  {'d', 'er', 'i', 's', 'n', 'o', 'w', 'l', 't', 'e', 'r', '_'}\n",
      "Corpus:  {'l o w _': 5, 'l o w e s t _': 2, 'n e w er _': 6, 'w i d er _': 3, 'n e w _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 2: Merged ('er', '_') -> er_, Count:9\n",
      "Vocab:  {'d', 'er', 'er_', 'i', 's', 'n', 'o', 'w', 'l', 't', 'e', 'r', '_'}\n",
      "Corpus:  {'l o w _': 5, 'l o w e s t _': 2, 'n e w er_': 6, 'w i d er_': 3, 'n e w _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 3: Merged ('n', 'e') -> ne, Count:8\n",
      "Vocab:  {'d', 'er', 'er_', 'i', 's', 'n', 'o', 'w', 'l', 't', 'ne', 'e', 'r', '_'}\n",
      "Corpus:  {'l o w _': 5, 'l o w e s t _': 2, 'ne w er_': 6, 'w i d er_': 3, 'ne w _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 4: Merged ('ne', 'w') -> new, Count:8\n",
      "Vocab:  {'d', 'er', 'er_', 'i', 's', 'n', 'o', 'w', 'l', 'new', 't', 'ne', 'e', 'r', '_'}\n",
      "Corpus:  {'l o w _': 5, 'l o w e s t _': 2, 'new er_': 6, 'w i d er_': 3, 'new _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 5: Merged ('l', 'o') -> lo, Count:7\n",
      "Vocab:  {'d', 'er', 'er_', 'i', 's', 'lo', 'n', 'o', 'w', 'l', 'new', 't', 'ne', 'e', 'r', '_'}\n",
      "Corpus:  {'lo w _': 5, 'lo w e s t _': 2, 'new er_': 6, 'w i d er_': 3, 'new _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 6: Merged ('lo', 'w') -> low, Count:7\n",
      "Vocab:  {'d', 'er', 'er_', 'i', 's', 'lo', 'n', 'o', 'w', 'low', 'l', 'new', 't', 'ne', 'e', 'r', '_'}\n",
      "Corpus:  {'low _': 5, 'low e s t _': 2, 'new er_': 6, 'w i d er_': 3, 'new _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 7: Merged ('new', 'er_') -> newer_, Count:6\n",
      "Vocab:  {'d', 'er', 'er_', 'i', 's', 'lo', 'n', 'newer_', 'o', 'w', 'low', 'l', 'new', 't', 'ne', 'e', 'r', '_'}\n",
      "Corpus:  {'low _': 5, 'low e s t _': 2, 'newer_': 6, 'w i d er_': 3, 'new _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 8: Merged ('low', '_') -> low_, Count:5\n",
      "Vocab:  {'er', 'low_', 'newer_', 'o', 'w', 'ne', 'e', 'd', 'i', 's', 'new', 'l', 'r', 'er_', '_', 'low', 'lo', 'n', 't'}\n",
      "Corpus:  {'low_': 5, 'low e s t _': 2, 'newer_': 6, 'w i d er_': 3, 'new _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 9: Merged ('w', 'i') -> wi, Count:3\n",
      "Vocab:  {'er', 'low_', 'newer_', 'o', 'w', 'ne', 'e', 'd', 'i', 's', 'wi', 'new', 'l', 'r', 'er_', '_', 'low', 'lo', 'n', 't'}\n",
      "Corpus:  {'low_': 5, 'low e s t _': 2, 'newer_': 6, 'wi d er_': 3, 'new _': 2}\n",
      "============================================================\n",
      "Next? y\n",
      "Step 10: Merged ('wi', 'd') -> wid, Count:3\n",
      "Vocab:  {'er', 'low_', 'newer_', 'o', 'w', 'ne', 'e', 'd', 'i', 's', 'wi', 'new', 'l', 'r', 'er_', '_', 'low', 'wid', 'lo', 'n', 't'}\n",
      "Corpus:  {'low_': 5, 'low e s t _': 2, 'newer_': 6, 'wid er_': 3, 'new _': 2}\n",
      "============================================================\n",
      "Next? y\n"
     ]
    }
   ],
   "source": [
    "training_corpus = {\n",
    "    'low!_': 5,\n",
    "    'lowest!_': 2,\n",
    "    'newer!_': 6,\n",
    "    'wider!_': 3,\n",
    "    'new!_': 2,\n",
    "}\n",
    "\n",
    "tokenizer = BPE()\n",
    "\n",
    "# Run BPE with 10 merge steps\n",
    "tokenizer.token_learner(training_corpus, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36887b",
   "metadata": {},
   "source": [
    "### Demo of GPT-2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b59db126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e50a5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3118, 71, 42661, 287, 17019, 676, 338, 8718, 12, 7217, 2208, 785, 48074]\n",
      "\n",
      "Tokenized Words/Subwords: ['Un', 'h', 'appiness', 'Ġin', 'Ġneural', 'ink', \"'s\", 'Ġhyper', '-', 'fast', 'Ġsuper', 'com', 'puting']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 tokenizer (which is similar to GPT-3's tokenizer in approach)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example text to tokenize\n",
    "text = \"Unhappiness in neuralink's hyper-fast supercomputing\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "# Print the tokens (which are integers)\n",
    "print(f\"Token IDs: {tokens}\\n\")\n",
    "\n",
    "# Print the individual tokens and their corresponding words/subwords\n",
    "tokenized_words = tokenizer.tokenize(text)\n",
    "print(f\"Tokenized Words/Subwords: {tokenized_words}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f770ed10",
   "metadata": {},
   "source": [
    "The special token Ġ before some words (like Ġin, Ġneural) represents a space. This indicates that these tokens appear at the beginning of words after a space in the input text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
