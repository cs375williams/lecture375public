{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75043cde",
   "metadata": {},
   "source": [
    "### Lecture 15\n",
    "Fine-tuning BERT for NER. \n",
    "\n",
    "Code adapted from: https://huggingface.co/docs/transformers/en/tasks/token_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32876df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a170f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorForTokenClassification, pipeline)\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8584b064",
   "metadata": {},
   "source": [
    "### 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00779c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with labeled NER BIO tags\n",
    "wnut = load_dataset(\"wnut_17\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45784c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of possible token labels \n",
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821f364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer into y=0, y=1, y=2 etc. \n",
    "label2id = {}\n",
    "id2label = {}\n",
    "for i, label in enumerate(label_list): \n",
    "    label2id[label] = i \n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0500013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-corporation': 1,\n",
       " 'I-corporation': 2,\n",
       " 'B-creative-work': 3,\n",
       " 'I-creative-work': 4,\n",
       " 'B-group': 5,\n",
       " 'I-group': 6,\n",
       " 'B-location': 7,\n",
       " 'I-location': 8,\n",
       " 'B-person': 9,\n",
       " 'I-person': 10,\n",
       " 'B-product': 11,\n",
       " 'I-product': 12}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "071efe1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'tokens': ['All',\n",
       "  'I',\n",
       "  \"'\",\n",
       "  've',\n",
       "  'been',\n",
       "  'doing',\n",
       "  'is',\n",
       "  'BINGE',\n",
       "  'watching',\n",
       "  'Rick',\n",
       "  'and',\n",
       "  'Morty',\n",
       "  'ðŸ˜‚'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example with the named entity \"Rick and Morty\"\n",
    "wnut[\"validation\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1864db4",
   "metadata": {},
   "source": [
    "### 2. Tokenization and pre-processing\n",
    "\n",
    "We'll load a smaller \"distilled\" BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94dfd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "672dfb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'all',\n",
       " 'i',\n",
       " \"'\",\n",
       " 've',\n",
       " 'been',\n",
       " 'doing',\n",
       " 'is',\n",
       " 'bing',\n",
       " '##e',\n",
       " 'watching',\n",
       " 'rick',\n",
       " 'and',\n",
       " 'mort',\n",
       " '##y',\n",
       " '[UNK]',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's how the tokenizer works \n",
    "example = wnut[\"validation\"][2]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb05ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], \n",
    "                                 truncation=True, \n",
    "                                 is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c827bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44cc73d85344ae5b19c8992d95e175d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1835c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to help us create batches \n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4dc52",
   "metadata": {},
   "source": [
    "### 3. Model & training\n",
    "\n",
    "We'll load a smaller \"distilled\" BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da8fd60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert/distilbert-base-uncased'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec09fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_name, \n",
    "                                                        num_labels=len(label2id), \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff7e9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do I specify the optimizer?! \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"saved_models\",      # where to save the trainined model 2 \n",
    "    learning_rate=2e-5,             # initial learning rate\n",
    "    per_device_train_batch_size=50, # batch sizes within SGD\n",
    "    per_device_eval_batch_size=50,\n",
    "    num_train_epochs=2,             # epoch = 1 time thru the training data \n",
    "    eval_strategy=\"epoch\",          # evaluates on the dev set each epoch\n",
    "    save_strategy=\"epoch\",          # saves a checkpoint of the model each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab8fad",
   "metadata": {},
   "source": [
    "What metrics do we want? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2026890",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\") # Some pre-computed token-level precision, recall, F1\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761b4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fda42939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 19:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.241080</td>\n",
       "      <td>0.604853</td>\n",
       "      <td>0.417464</td>\n",
       "      <td>0.493984</td>\n",
       "      <td>0.945769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.233566</td>\n",
       "      <td>0.652406</td>\n",
       "      <td>0.437799</td>\n",
       "      <td>0.523980</td>\n",
       "      <td>0.947613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherinekeith/opt/miniconda3/envs/cs375/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=136, training_loss=0.13805177632500143, metrics={'train_runtime': 1159.4324, 'train_samples_per_second': 5.855, 'train_steps_per_second': 0.117, 'total_flos': 102032801108760.0, 'train_loss': 0.13805177632500143, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Takes about 20 minutes on Katie's Apple M1 CPUs... \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4effb4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('saved_models/tokenizer_config.json',\n",
       " 'saved_models/special_tokens_map.json',\n",
       " 'saved_models/vocab.txt',\n",
       " 'saved_models/added_tokens.json',\n",
       " 'saved_models/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the final model and tokenizer \n",
    "model.save_pretrained(\"saved_models\")\n",
    "tokenizer.save_pretrained(\"saved_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0d24b",
   "metadata": {},
   "source": [
    "### 4. Test-time inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e08ea18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"ner\", model=\"saved_models\", tokenizer=\"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a083fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-person',\n",
       "  'score': 0.7615155,\n",
       "  'index': 1,\n",
       "  'word': 'katie',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity': 'I-person',\n",
       "  'score': 0.5479589,\n",
       "  'index': 2,\n",
       "  'word': 'keith',\n",
       "  'start': 6,\n",
       "  'end': 11},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.27801132,\n",
       "  'index': 7,\n",
       "  'word': 'williams',\n",
       "  'start': 30,\n",
       "  'end': 38},\n",
       " {'entity': 'I-location',\n",
       "  'score': 0.24160397,\n",
       "  'index': 8,\n",
       "  'word': 'college',\n",
       "  'start': 39,\n",
       "  'end': 46}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Katie Keith is a professor at Williams College!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
